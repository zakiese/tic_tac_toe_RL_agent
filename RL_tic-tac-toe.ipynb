{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd0901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac42c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeEnv, self).__init__()\n",
    "        \n",
    "        # 9 discrete positions to place X or O\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "\n",
    "        # Observation: 9 cells with values {0: empty, 1: agent, -1: opponent}\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(9,), dtype=np.int8)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done or self.board[action] != 0:\n",
    "            return self.board.copy(), -10, True, False, {}  # illegal move\n",
    "\n",
    "        # Agent (1) moves\n",
    "        self.board[action] = 1\n",
    "        reward, terminated = self.check_winner(1)\n",
    "        if terminated:\n",
    "            return self.board.copy(), reward, True, False, {}\n",
    "\n",
    "        # Opponent (-1) random move\n",
    "        available = np.where(self.board == 0)[0]\n",
    "        if len(available) > 0:\n",
    "            opp_action = np.random.choice(available)\n",
    "            self.board[opp_action] = -1\n",
    "            reward, terminated = self.check_winner(-1)\n",
    "            if terminated:\n",
    "                return self.board.copy(), -1, True, False, {}\n",
    "\n",
    "        # Draw?\n",
    "        if np.all(self.board != 0):\n",
    "            return self.board.copy(), 0.5, True, False, {}\n",
    "\n",
    "        return self.board.copy(), 0, False, False, {}\n",
    "\n",
    "    def check_winner(self, player):\n",
    "        b = self.board.reshape(3, 3)\n",
    "        for i in range(3):\n",
    "            if np.all(b[i, :] == player) or np.all(b[:, i] == player):\n",
    "                return (1 if player == 1 else -1), True\n",
    "        if np.all(np.diag(b) == player) or np.all(np.diag(np.fliplr(b)) == player):\n",
    "            return (1 if player == 1 else -1), True\n",
    "        return 0, False\n",
    "\n",
    "    def render(self):\n",
    "        symbols = {1: 'X', -1: 'O', 0: '.'}\n",
    "        b = self.board.reshape(3, 3)\n",
    "        for row in b:\n",
    "            print(\" \".join(symbols[cell] for cell in row))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c251bbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "\n",
      "X . .\n",
      ". . O\n",
      ". . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv()\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "obs, reward, done, _, _ = env.step(0)  # Agent places at position 0\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc46ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.1):\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return [self.q_table[(tuple(state), a)] for a in range(9)]\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        qs = self.get_qs(state)\n",
    "        # Choose max Q only among available actions\n",
    "        qs_avail = {a: qs[a] for a in available_actions}\n",
    "        return max(qs_avail, key=qs_avail.get)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, available_actions):\n",
    "        max_future_q = max([self.q_table[(tuple(next_state), a)] for a in available_actions]) if not done else 0\n",
    "        current_q = self.q_table[(tuple(state), action)]\n",
    "\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
    "        self.q_table[(tuple(state), action)] = new_q\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426acecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000 - Win: 4315, Draw: 296, Loss: 389\n",
      "Episode 10000 - Win: 4550, Draw: 239, Loss: 211\n",
      "Episode 15000 - Win: 4587, Draw: 246, Loss: 167\n",
      "Episode 20000 - Win: 4620, Draw: 223, Loss: 157\n",
      "Episode 25000 - Win: 4635, Draw: 207, Loss: 158\n",
      "Episode 30000 - Win: 4633, Draw: 213, Loss: 154\n",
      "Episode 35000 - Win: 4656, Draw: 192, Loss: 152\n",
      "Episode 40000 - Win: 4628, Draw: 217, Loss: 155\n",
      "Episode 45000 - Win: 4675, Draw: 187, Loss: 138\n",
      "Episode 50000 - Win: 4709, Draw: 186, Loss: 105\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "num_episodes = 50000\n",
    "win_count = 0\n",
    "draw_count = 0\n",
    "lose_count = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        available = np.where(state == 0)[0]\n",
    "        action = agent.choose_action(state, available)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        next_available = np.where(next_state == 0)[0]\n",
    "        agent.update(state, action, reward, next_state, done, next_available)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                win_count += 1\n",
    "            elif reward == 0.5:\n",
    "                draw_count += 1\n",
    "            elif reward == -1:\n",
    "                lose_count += 1\n",
    "\n",
    "    # Print every 5000 episodes\n",
    "    if (episode + 1) % 5000 == 0:\n",
    "        print(f\"Episode {episode+1} - Win: {win_count}, Draw: {draw_count}, Loss: {lose_count}\")\n",
    "        win_count = draw_count = lose_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880701eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, episodes=1000):\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            available = np.where(state == 0)[0]\n",
    "            action = agent.choose_action(state, available)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    wins += 1\n",
    "                elif reward == 0.5:\n",
    "                    draws += 1\n",
    "                elif reward == -1:\n",
    "                    losses += 1\n",
    "                break\n",
    "\n",
    "    total = wins + draws + losses\n",
    "    print(f\"Out of {total} games:\")\n",
    "    print(f\" Wins: {wins}\")\n",
    "    print(f\" Draws: {draws}\")\n",
    "    print(f\" Losses: {losses}\")\n",
    "    print(f\" Win Rate: {wins / total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b55b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1000 games:\n",
      " Wins: 927\n",
      " Draws: 45\n",
      " Losses: 28\n",
      " Win Rate: 92.70%\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1088e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_vs_agent(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    while True:\n",
    "        # Agent move\n",
    "        available = np.where(state == 0)[0]\n",
    "        action = agent.choose_action(state, available)\n",
    "        print(f\"Agent (X) chooses: {action}\")\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                print(\"Agent wins!\")\n",
    "            elif reward == 0.5:\n",
    "                print(\"Draw!\")\n",
    "            elif reward == -1:\n",
    "                print(\"You win!\")\n",
    "            break\n",
    "\n",
    "        # Human move\n",
    "        while True:\n",
    "            try:\n",
    "                user_action = int(input(\"Your move (0-8): \"))\n",
    "                if state[user_action] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid move! Cell is already taken.\")\n",
    "            except:\n",
    "                print(\"Please enter a valid number from 0 to 8.\")\n",
    "\n",
    "        # Apply human move\n",
    "        row, col = divmod(user_action, 3)\n",
    "        env.board[row * 3 + col] = -1  # Human is -1\n",
    "        state = env.board.copy()\n",
    "        env.render()\n",
    "\n",
    "        # Check if human wins\n",
    "        r, done_flag = env.check_winner(-1)\n",
    "        if done_flag:\n",
    "            if r == -1:\n",
    "                print(\"You win!\")\n",
    "            else:\n",
    "                print(\"Draw!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322618e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "\n",
      "Agent (X) chooses: 0\n",
      "X . O\n",
      ". . .\n",
      ". . .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X O O\n",
      ". . .\n",
      ". . .\n",
      "\n",
      "Agent (X) chooses: 6\n",
      "X O O\n",
      ". O .\n",
      "X . .\n",
      "\n",
      "X O O\n",
      ". O O\n",
      "X . .\n",
      "\n",
      "Agent (X) chooses: 8\n",
      "X O O\n",
      ". O O\n",
      "X O X\n",
      "\n",
      "You win!\n"
     ]
    }
   ],
   "source": [
    "play_vs_agent(agent, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
